{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678f938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_ALLOW_INSECURE_GATEWAY\"] = \"1\"\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr\n",
    "from pyspark.sql.types import StructType, IntegerType, StringType, TimestampType\n",
    "\n",
    "\n",
    "DB_URL = \"jdbc:postgresql://postgres:5432/postgres\"\n",
    "DB_USER = \"myuser\"\n",
    "DB_PASS = \"myuserpass\"\n",
    "DB_TABLE = \"kafka_data_05\"\n",
    "BAD_ROWS_PATH = \"/tmp/bad_rows_05\"\n",
    "CHECKPOINT_PATH = \"/tmp/checkpoints/kafka-to-pgsql_05\"\n",
    "\n",
    "\n",
    "# SparkSession z obsługą Kafka + JSON\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"StreamStreamJoin\")\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Schematy danych\n",
    "orders_schema = (\n",
    "    StructType()\n",
    "    .add(\"order_id\", IntegerType())\n",
    "    .add(\"user_id\", IntegerType())\n",
    "    .add(\"amount\", IntegerType())\n",
    "    .add(\"timestamp\", TimestampType())\n",
    ")\n",
    "\n",
    "users_schema = (\n",
    "    StructType()\n",
    "    .add(\"user_id\", IntegerType())\n",
    "    .add(\"user_name\", StringType())\n",
    "    .add(\"timestamp\", TimestampType())\n",
    ")\n",
    "\n",
    "# Strumień zamówień\n",
    "orders = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "    .option(\"subscribe\", \"spark-lab5-topic-orders\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "orders_df = (\n",
    "    orders.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json(col(\"value\"), orders_schema).alias(\"order\"))\n",
    "    .select(\"order.*\")\n",
    "    .withWatermark(\"timestamp\", \"45 seconds\")\n",
    "    .alias(\"orders_df\")\n",
    ")\n",
    "\n",
    "# Strumień użytkowników\n",
    "users = (\n",
    "    spark.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka_streaming_lab:9092\")\n",
    "    .option(\"subscribe\", \"spark-lab5-topic-users\")\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .load()\n",
    ")\n",
    "\n",
    "users_df = (\n",
    "    users.selectExpr(\"CAST(value AS STRING)\")\n",
    "    .select(from_json(col(\"value\"), users_schema).alias(\"user\"))\n",
    "    .select(\"user.*\")\n",
    "    .withWatermark(\"timestamp\", \"60 seconds\")\n",
    "    .alias(\"users_df\")\n",
    ")\n",
    "\n",
    "# Strumieniowe łączenie po user_id\n",
    "joined = orders_df.join(\n",
    "    users_df,\n",
    "    expr(\"\"\"\n",
    "        orders_df.user_id = users_df.user_id AND\n",
    "        orders_df.timestamp BETWEEN users_df.timestamp - interval 15 seconds AND users_df.timestamp + interval 15 seconds\n",
    "    \"\"\")\n",
    ")\n",
    "\n",
    "joined = joined.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"orders_df.user_id\").alias(\"user_id\"),\n",
    "    col(\"amount\"),\n",
    "    col(\"orders_df.timestamp\").alias(\"order_timestamp\"),\n",
    "    col(\"user_name\"),\n",
    "    col(\"users_df.timestamp\").alias(\"user_timestamp\")\n",
    ")\n",
    "\n",
    "# Zapisywanie do PostgreSQL\n",
    "def write_to_postgres(batch_df, batch_id):\n",
    "    (\n",
    "        batch_df.write\n",
    "        .format(\"jdbc\")\n",
    "        .option(\"url\", DB_URL)\n",
    "        .option(\"dbtable\", DB_TABLE)\n",
    "        .option(\"user\", DB_USER)\n",
    "        .option(\"password\", DB_PASS)\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .mode(\"append\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "# Zapis jako foreachBatch\n",
    "query = (\n",
    "    joined.writeStream\n",
    "    .foreachBatch(write_to_postgres)\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "# Zapis do Kafka\n",
    "# (\n",
    "#     kafka_ready.write\n",
    "#     .format(\"kafka\") \\\n",
    "#     .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "#     .option(\"topic\", \"test-topic\")\n",
    "#     .save()\n",
    "# )\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df68b267-54c4-4588-849b-857ea146eeb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
